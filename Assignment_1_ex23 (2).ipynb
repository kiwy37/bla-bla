{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Team: Balogh Szilard, Bajan Ramona-Maria, Popa Sebastian\n",
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw DataFrame created with 8654 entries.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cocoa</td>\n",
       "      <td>Showers continued throughout the week in\\nthe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>earn</td>\n",
       "      <td>Champion Products Inc said its\\nboard of direc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acq</td>\n",
       "      <td>Computer Terminal Systems Inc said\\nit has com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>earn</td>\n",
       "      <td>Shr 34 cts vs 1.19 dlrs\\n    Net 807,000 vs 2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>earn</td>\n",
       "      <td>Oper shr loss two cts vs profit seven cts\\n   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic                                               Body\n",
       "0  cocoa  Showers continued throughout the week in\\nthe ...\n",
       "1   earn  Champion Products Inc said its\\nboard of direc...\n",
       "2    acq  Computer Terminal Systems Inc said\\nit has com...\n",
       "3   earn  Shr 34 cts vs 1.19 dlrs\\n    Net 807,000 vs 2,...\n",
       "4   earn  Oper shr loss two cts vs profit seven cts\\n   ..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "folder_path = r\"reuters21578\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".sgm\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        with open(file_path, \"r\", encoding=\"latin1\") as file:\n",
    "            content = file.read()\n",
    "            soup = BeautifulSoup(content, \"html.parser\")\n",
    "            \n",
    "            for reuters in soup.find_all(\"reuters\"):\n",
    "                topics = reuters.find(\"topics\")\n",
    "                body = reuters.find(\"body\")\n",
    "                \n",
    "                if topics and body and len(topics.contents) == 1:\n",
    "                    topic_text = topics.get_text(strip=True)\n",
    "                    body_text = body.get_text(separator=\" \", strip=True)\n",
    "                    \n",
    "                    data.append({\"Topic\": topic_text, \"Body\": body_text})\n",
    "\n",
    "raw_df = pd.DataFrame(data)\n",
    "print(f\"Raw DataFrame created with {len(raw_df)} entries.\")\n",
    "\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame preprocessed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Body Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cocoa</td>\n",
       "      <td>[showers, continued, throughout, the, week, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>earn</td>\n",
       "      <td>[champion, products, inc, said, its, board, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acq</td>\n",
       "      <td>[computer, terminal, systems, inc, said, it, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>earn</td>\n",
       "      <td>[shr, cts, vs, dlrs, net, vs, assets, mln, vs,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>earn</td>\n",
       "      <td>[oper, shr, loss, two, cts, vs, profit, seven,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic                                        Body Tokens\n",
       "0  cocoa  [showers, continued, throughout, the, week, in...\n",
       "1   earn  [champion, products, inc, said, its, board, of...\n",
       "2    acq  [computer, terminal, systems, inc, said, it, h...\n",
       "3   earn  [shr, cts, vs, dlrs, net, vs, assets, mln, vs,...\n",
       "4   earn  [oper, shr, loss, two, cts, vs, profit, seven,..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_bodies = []\n",
    "\n",
    "for body in raw_df[\"Body\"]:\n",
    "    cleaned_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", body)\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    tokens = cleaned_text.split()\n",
    "    processed_bodies.append(tokens)\n",
    "\n",
    "raw_df[\"Body Tokens\"] = processed_bodies\n",
    "\n",
    "processed_df = raw_df.drop(columns=[\"Body\"])\n",
    "\n",
    "print(f\"DataFrame preprocessed.\")\n",
    "\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Body Tokens</th>\n",
       "      <th>Body Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cocoa</td>\n",
       "      <td>[showers, continued, throughout, the, week, in...</td>\n",
       "      <td>showers continued throughout the week in the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>earn</td>\n",
       "      <td>[champion, products, inc, said, its, board, of...</td>\n",
       "      <td>champion products inc said its board of direct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acq</td>\n",
       "      <td>[computer, terminal, systems, inc, said, it, h...</td>\n",
       "      <td>computer terminal systems inc said it has comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>earn</td>\n",
       "      <td>[shr, cts, vs, dlrs, net, vs, assets, mln, vs,...</td>\n",
       "      <td>shr cts vs dlrs net vs assets mln vs mln depos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>earn</td>\n",
       "      <td>[oper, shr, loss, two, cts, vs, profit, seven,...</td>\n",
       "      <td>oper shr loss two cts vs profit seven cts oper...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic                                        Body Tokens  \\\n",
       "0  cocoa  [showers, continued, throughout, the, week, in...   \n",
       "1   earn  [champion, products, inc, said, its, board, of...   \n",
       "2    acq  [computer, terminal, systems, inc, said, it, h...   \n",
       "3   earn  [shr, cts, vs, dlrs, net, vs, assets, mln, vs,...   \n",
       "4   earn  [oper, shr, loss, two, cts, vs, profit, seven,...   \n",
       "\n",
       "                                           Body Text  \n",
       "0  showers continued throughout the week in the b...  \n",
       "1  champion products inc said its board of direct...  \n",
       "2  computer terminal systems inc said it has comp...  \n",
       "3  shr cts vs dlrs net vs assets mln vs mln depos...  \n",
       "4  oper shr loss two cts vs profit seven cts oper...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df[\"Body Text\"] = processed_df[\"Body Tokens\"].apply(lambda tokens: \" \".join(tokens))\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_count = count_vectorizer.fit_transform(processed_df[\"Body Text\"])\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(processed_df[\"Body Text\"])\n",
    "\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\BajanRamonaMaria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       shower continu throughout week bahia cocoa zon...\n",
      "1       champion product inc said board director appro...\n",
      "2       comput termin system inc said complet sale sha...\n",
      "3       shr ct vs dlr net vs asset mln vs mln deposit ...\n",
      "4       oper shr loss two ct vs profit seven ct oper s...\n",
      "                              ...                        \n",
      "8649    soviet union agre suppli iran refin oil produc...\n",
      "8650    chase corp ltd chcaw said make offer fullypaid...\n",
      "8651    japanindiapakistangulfjapan ship confer said w...\n",
      "8652    soviet union industri output grow slower pace ...\n",
      "8653    six black miner kill two injur rock fall three...\n",
      "Name: BOW Text, Length: 8654, dtype: object\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(tokens):\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "processed_df[\"BOW Text\"] = processed_df[\"Body Tokens\"].apply(preprocess_text)\n",
    "\n",
    "print(processed_df[\"BOW Text\"])\n",
    "\n",
    "bow_vectorizer = CountVectorizer()\n",
    "X_bow = bow_vectorizer.fit_transform(processed_df[\"BOW Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(processed_df[\"Topic\"])\n",
    "\n",
    "X_train_idx, X_test_idx, y_train, y_test = train_test_split(\n",
    "    range(len(y)), y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train_count = X_count[X_train_idx]\n",
    "X_test_count = X_count[X_test_idx]\n",
    "\n",
    "X_train_tfidf = X_tfidf[X_train_idx]\n",
    "X_test_tfidf = X_tfidf[X_test_idx]\n",
    "\n",
    "X_train_bow = X_bow[X_train_idx]\n",
    "X_test_bow = X_bow[X_test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "metrics = [\"euclidean\", \"cosine\", \"manhattan\",]\n",
    "k_values = [3, 5, 7]\n",
    "\n",
    "def train_knn(X_train, X_test, y_train, y_test, metric, k):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric=metric, weights='distance')\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: euclidean, k: 3\n",
      "Raw Frequencies Accuracy: 0.8070\n",
      "TF-IDF Accuracy: 0.8891\n",
      "BOW Accuracy: 0.8348\n",
      "----------------------------------------\n",
      "Metric: euclidean, k: 5\n",
      "Raw Frequencies Accuracy: 0.8070\n",
      "TF-IDF Accuracy: 0.9006\n",
      "BOW Accuracy: 0.8313\n",
      "----------------------------------------\n",
      "Metric: euclidean, k: 7\n",
      "Raw Frequencies Accuracy: 0.8070\n",
      "TF-IDF Accuracy: 0.9006\n",
      "BOW Accuracy: 0.8284\n",
      "----------------------------------------\n",
      "Metric: cosine, k: 3\n",
      "Raw Frequencies Accuracy: 0.8562\n",
      "TF-IDF Accuracy: 0.8897\n",
      "BOW Accuracy: 0.9029\n",
      "----------------------------------------\n",
      "Metric: cosine, k: 5\n",
      "Raw Frequencies Accuracy: 0.8585\n",
      "TF-IDF Accuracy: 0.9029\n",
      "BOW Accuracy: 0.9035\n",
      "----------------------------------------\n",
      "Metric: cosine, k: 7\n",
      "Raw Frequencies Accuracy: 0.8533\n",
      "TF-IDF Accuracy: 0.9041\n",
      "BOW Accuracy: 0.9018\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MASTER_AN_I_SEM_II\\DM\\TEMA1\\venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"d:\\MASTER_AN_I_SEM_II\\DM\\TEMA1\\venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"C:\\Users\\BajanRamonaMaria\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 556, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BajanRamonaMaria\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1038, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BajanRamonaMaria\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1550, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: manhattan, k: 3\n",
      "Raw Frequencies Accuracy: 0.7614\n",
      "TF-IDF Accuracy: 0.4749\n",
      "BOW Accuracy: 0.7383\n",
      "----------------------------------------\n",
      "Metric: manhattan, k: 5\n",
      "Raw Frequencies Accuracy: 0.7603\n",
      "TF-IDF Accuracy: 0.4685\n",
      "BOW Accuracy: 0.7210\n",
      "----------------------------------------\n",
      "Metric: manhattan, k: 7\n",
      "Raw Frequencies Accuracy: 0.7481\n",
      "TF-IDF Accuracy: 0.4633\n",
      "BOW Accuracy: 0.6880\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for metric in metrics:\n",
    "    for k in k_values:\n",
    "        acc_count = train_knn(X_train_count, X_test_count, y_train, y_test, metric, k)\n",
    "        acc_tfidf = train_knn(X_train_tfidf, X_test_tfidf, y_train, y_test, metric, k)\n",
    "        acc_bow = train_knn(X_train_bow, X_test_bow, y_train, y_test, metric, k)\n",
    "        \n",
    "        print(f\"Metric: {metric}, k: {k}\")\n",
    "        print(f\"Raw Frequencies Accuracy: {acc_count:.4f}\")\n",
    "        print(f\"TF-IDF Accuracy: {acc_tfidf:.4f}\")\n",
    "        print(f\"BOW Accuracy: {acc_bow:.4f}\")\n",
    "        print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: manhattan, k: 3\n",
      "Raw Frequencies Accuracy: 0.9041\n",
      "TF-IDF Accuracy: 0.7163\n",
      "BOW Accuracy: 0.9203\n",
      "----------------------------------------\n",
      "Metric: manhattan, k: 5\n",
      "Raw Frequencies Accuracy: 0.9041\n",
      "TF-IDF Accuracy: 0.7129\n",
      "BOW Accuracy: 0.9191\n",
      "----------------------------------------\n",
      "Metric: manhattan, k: 7\n",
      "Raw Frequencies Accuracy: 0.9076\n",
      "TF-IDF Accuracy: 0.7152\n",
      "BOW Accuracy: 0.9237\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X_count = normalize(X_count, norm='l1', axis=1)\n",
    "X_tfidf = normalize(X_tfidf, norm='l1', axis=1)\n",
    "X_bow = normalize(X_bow, norm='l1', axis=1)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(processed_df[\"Topic\"])\n",
    "\n",
    "X_train_idx, X_test_idx, y_train, y_test = train_test_split(\n",
    "    range(len(y)), y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train_count = X_count[X_train_idx]\n",
    "X_test_count = X_count[X_test_idx]\n",
    "\n",
    "X_train_tfidf = X_tfidf[X_train_idx]\n",
    "X_test_tfidf = X_tfidf[X_test_idx]\n",
    "\n",
    "X_train_bow = X_bow[X_train_idx]\n",
    "X_test_bow = X_bow[X_test_idx]\n",
    "\n",
    "metrics = [\"manhattan\",]\n",
    "k_values = [3, 5, 7]\n",
    "\n",
    "def train_knn(X_train, X_test, y_train, y_test, metric, k):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric=metric, weights='distance')\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "for metric in metrics:\n",
    "    for k in k_values:\n",
    "        acc_count = train_knn(X_train_count, X_test_count, y_train, y_test, metric, k)\n",
    "        acc_tfidf = train_knn(X_train_tfidf, X_test_tfidf, y_train, y_test, metric, k)\n",
    "        acc_bow = train_knn(X_train_bow, X_test_bow, y_train, y_test, metric, k)\n",
    "        \n",
    "        print(f\"Metric: {metric}, k: {k}\")\n",
    "        print(f\"Raw Frequencies Accuracy: {acc_count:.4f}\")\n",
    "        print(f\"TF-IDF Accuracy: {acc_tfidf:.4f}\")\n",
    "        print(f\"BOW Accuracy: {acc_bow:.4f}\")\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this experiment we aimed to demonstrate the importance of distances and similarities in the context of the Reuters Text Categorization Collection, using different metrics (cosine, manhattan, euclidian), different vectorization strategies (raw word frequencies, TF-IDF and BOW), using different values for 'k' (3, 5, 7) in the KNeighborsClassifier algorithm. \n",
    "\n",
    "##### We observed that, overall, the raw word frequency vectorization approach gives the poorest results as it only counts the number of times each word appears in a document, without weighing the importance of a word. Common words like \"the\", \"and\" may dominate over less frequent but significant words, the overall accuracy rate being about 0.8. The BoW approach does better than the previously mentioned strategy, as it gets rid of the stopwords and uses a stemmer to reduce the words to their root, the overall accuracy rate being 0.85. Overall, the best results came with the TF-IDF vectorization strategy, which combines how often a word appears in a document with how unique the word is across all documents, which 4 times out of 9 came with an accuracy greater than 0.9. \n",
    "\n",
    "##### As for the metrics, we observed that cosine distance works best overall. Our biggest accuracy was for the cosine distance combined with the TF-IDF and k = 7, which is 0.9041. Similarly, for k = 5, the same metric-vectorization strategy combination gives a result of 0.9029. In our experiment, for the cosine metric, out of the 9 accuracies, 5 are above the 0.9 threshold, and all 9 of them are above the 0.85 threshold. Out of the remaining 18 accuracies, 16 are below the 0.9 threshold. The second best metric is the euclidian distance, for which all 9 accuracies are above 0.8 and 2 are above the 0.9 threshold. The manhattan distance suffers due to the \"curse of dimensionality\" and is generally not used in contextes where text data is being processed. However, after applying the L1 normalization, which is also called Manhattan normalization, the accuracies for manhattan distance improve significantly. On the other hand, the results of TF-IDF remain poorer than those of row word frequencies or BoW, which is due to the fact that after normalization each document is scaled, which interferes with the algorithm of TD-IDF which relies on rare word importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(processed_df[\"Topic\"])\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_count = count_vectorizer.fit_transform(processed_df[\"BOW Text\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_count, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Original Data: 0.9492\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "accuracy_original = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on Original Data: {accuracy_original:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on PCA-Reduced Data: 0.9076\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=100)\n",
    "X_train_pca = pca.fit_transform(X_train.toarray())\n",
    "X_test_pca = pca.transform(X_test.toarray())\n",
    "\n",
    "logreg_pca = LogisticRegression(max_iter=1000)\n",
    "logreg_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = logreg_pca.predict(X_test_pca)\n",
    "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
    "print(f\"Accuracy on PCA-Reduced Data: {accuracy_pca:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on t-SNE-Reduced Data: 0.5425\n"
     ]
    }
   ],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_train_tsne = tsne.fit_transform(X_train.toarray())\n",
    "X_test_tsne = tsne.fit_transform(X_test.toarray())\n",
    "\n",
    "logreg_tsne = LogisticRegression(max_iter=60000)\n",
    "logreg_tsne.fit(X_train_tsne, y_train)\n",
    "y_pred_tsne = logreg_tsne.predict(X_test_tsne)\n",
    "accuracy_tsne = accuracy_score(y_test, y_pred_tsne)\n",
    "print(f\"Accuracy on t-SNE-Reduced Data: {accuracy_tsne:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this experiment, we took the high dimensional dataset obtained using the BoW vectorization strategy, then created a classification using logistic regression. \n",
    "##### First, we classified the whole dataset, after which we reduced the data using PCA and TSNE respectively. As we expected, we obtained the highest accuracy with the non-reduced dataset, which was about 0.95. \n",
    "##### The second best accuracy came from the PCA approach. It makes sense that this approach cannot be as accurate as the first one, because reducing thousands of dimensions to a few hundred means that some important words are lost. The accuracy depends greatly on the number of principal components specified. \n",
    "##### For instance, setting this value to 100 got us an accuracy of about 0.90. If we set the number of components to a higher value, the accuracy increases, if we do the opposite it decreases. It is worth noting that the purpose of reduction techniques is to increase the processing time of the dataset (in our case the classification), and therefore, the value of n_components cannot be set to a value that is too great (for example, we tried setting it to 500, which resulted in an accuracy of about 0.94), because the computation time will take just as much as if we didn't reduce our dataset at all. As for TSNE, it performed very poorly. It is caused by the fact that reducing the data to only 2D, the classifier lost almost all usefult information. \n",
    "##### TSNE is designed for visualization, not classification. This explain the huge drop to 0.44 accuracy. It is also worth noting that this approach takes the most time, more than 1 minute and we needed to set the max_iter value to 60000. We tried lower values, but it resulted in an error saying that the model didn't manage to converge. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
