{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70483921",
   "metadata": {},
   "source": [
    "Balogh Szilard, Bajan Ramona-Maria, Popa Sebastian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0425588",
   "metadata": {},
   "source": [
    "Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "98a46375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from math import comb\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2db56544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_binary_matrix(M, N, p):\n",
    "    return np.random.binomial(1, p, (M, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9c4a2982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_candidates(frequent_itemsets, k):\n",
    "    candidates = set()\n",
    "    itemsets_list = list(map(sorted, frequent_itemsets))  # Sort items to ensure the order\n",
    "\n",
    "    for i in range(len(itemsets_list)):\n",
    "        for j in range(i + 1, len(itemsets_list)):\n",
    "            # Check if the first k-2 items are the same\n",
    "            if itemsets_list[i][:-1] == itemsets_list[j][:-1]:\n",
    "                # Join the two itemsets\n",
    "                candidate_set = set(itemsets_list[i]).union(set(itemsets_list[j]))\n",
    "                if len(candidate_set) == k:\n",
    "                    candidates.add(tuple(sorted(candidate_set)))\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "460563e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashTreeNode:\n",
    "    def __init__(self, is_leaf=True, bucket_size=3):\n",
    "        self.is_leaf = is_leaf\n",
    "        self.buckets = {} if not is_leaf else []\n",
    "        self.bucket_size = bucket_size if is_leaf else None\n",
    "        self.support = {}  # only meaningful in leaf nodes\n",
    "\n",
    "class HashTree:\n",
    "    def __init__(self, k, hash_func=lambda x: x % 3, bucket_size=3):\n",
    "        self.root = HashTreeNode()\n",
    "        self.k = k\n",
    "        self.bucket_size = bucket_size\n",
    "        self.hash_func = hash_func\n",
    "\n",
    "    def insert(self, tuple_value):\n",
    "        self._insert_recursive(self.root, tuple_value, level=1)\n",
    "\n",
    "    def _insert_recursive(self, node, tuple_value, level):\n",
    "        if node.is_leaf:\n",
    "            if tuple_value not in node.support:\n",
    "                if len(node.buckets) < node.bucket_size or level > self.k:\n",
    "                    node.buckets.append(tuple_value)\n",
    "                    node.support[tuple_value] = 0\n",
    "                else:\n",
    "                    self._split_node(node, level)\n",
    "                    self._insert_recursive(node, tuple_value, level)\n",
    "            return\n",
    "\n",
    "        key = self.hash_func(tuple_value[level - 1])\n",
    "        if key not in node.buckets:\n",
    "            node.buckets[key] = HashTreeNode()\n",
    "        self._insert_recursive(node.buckets[key], tuple_value, level + 1)\n",
    "\n",
    "    def _split_node(self, node, level):\n",
    "        new_node = HashTreeNode(is_leaf=False)\n",
    "        for t in node.buckets:\n",
    "            key = self.hash_func(t[level - 1])\n",
    "            if key not in new_node.buckets:\n",
    "                new_node.buckets[key] = HashTreeNode()\n",
    "            new_node.buckets[key].buckets.append(t)\n",
    "            new_node.buckets[key].support[t] = node.support[t]\n",
    "        node.is_leaf = False\n",
    "        node.buckets = new_node.buckets\n",
    "        node.support = None\n",
    "\n",
    "    def insert_candidates(self, candidates):\n",
    "        for candidate in candidates:\n",
    "            self.insert(candidate)\n",
    "\n",
    "    def count_occurrences(self, transaction):\n",
    "        for subset in combinations(transaction, self.k):\n",
    "            self._count_single_subset(self.root, subset, level=1)\n",
    "\n",
    "    def _count_single_subset(self, node, subset, level):\n",
    "        if node.is_leaf:\n",
    "            if subset in node.support:\n",
    "                node.support[subset] += 1\n",
    "            return\n",
    "        key = self.hash_func(subset[level - 1])\n",
    "        if key in node.buckets:\n",
    "            self._count_single_subset(node.buckets[key], subset, level + 1)\n",
    "\n",
    "    def get_subsets(transaction, k):\n",
    "        if len(transaction) < k:\n",
    "            return []\n",
    "        return combinations(transaction, k)  \n",
    "\n",
    "    def get_supported_candidates(self, min_support):\n",
    "        result = []\n",
    "\n",
    "        def collect(node):\n",
    "            if node.is_leaf:\n",
    "                result.extend([item for item, count in node.support.items() if count >= min_support])\n",
    "            else:\n",
    "                for child in node.buckets.values():\n",
    "                    collect(child)\n",
    "\n",
    "        collect(self.root)\n",
    "        return result\n",
    "\n",
    "    def get_leaf_statistics(self):\n",
    "        leaf_count = 0\n",
    "        leaf_sizes = []\n",
    "\n",
    "        def count(node):\n",
    "            nonlocal leaf_count\n",
    "            if node.is_leaf:\n",
    "                leaf_count += 1\n",
    "                leaf_sizes.append(len(node.buckets))\n",
    "            else:\n",
    "                for child in node.buckets.values():\n",
    "                    count(child)\n",
    "\n",
    "        count(self.root)\n",
    "        return leaf_count, leaf_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "dfe6098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_using_indices(matrix: np.ndarray, min_support, verbose=False):\n",
    "    num_transactions, num_items = matrix.shape\n",
    "\n",
    "    # Step 1: Count the support for each individual item (using indices)\n",
    "    support = matrix.sum(axis=0)\n",
    "    single_items = {index: support[index] for index in range(num_items)}\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"1-itemsets: {single_items}\")\n",
    "\n",
    "    # Step 2: Remove items below min_support\n",
    "    single_items = {k: v for k, v in single_items.items() if v >= min_support}\n",
    "\n",
    "    # Convert frequent single items to a list of sets containing indices\n",
    "    frequent_itemsets = [tuple([index]) for index in single_items.keys()]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"1-itemsets above min_support: {len(frequent_itemsets)}\")\n",
    "\n",
    "    all_frequent_itemsets = []\n",
    "    all_frequent_itemsets.extend(frequent_itemsets)\n",
    "\n",
    "    k = 2\n",
    "    while frequent_itemsets:\n",
    "        # Step 3: Generate candidate itemsets of length k using indices\n",
    "        candidates = create_candidates(frequent_itemsets, k)\n",
    "\n",
    "        # Step 4: Count support of candidates using the binary matrix\n",
    "        candidate_support_counts = {candidate: 0 for candidate in candidates}\n",
    "\n",
    "        for candidate in candidates:\n",
    "            # Create a mask for rows that contain the candidate itemset\n",
    "            indices = list(candidate)\n",
    "            mask = matrix[:, indices].all(axis=1)  # All items must be present\n",
    "\n",
    "            # Count how many transactions contain the candidate itemset\n",
    "            candidate_support_counts[candidate] = np.sum(mask)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{k}-itemsets candidates: {len(candidate_support_counts)}\")\n",
    "\n",
    "        # Step 5: Prune non-frequent itemsets\n",
    "        frequent_itemsets = [c for c, count in candidate_support_counts.items() if count >= min_support]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{k}-itemsets above min_support: {len(frequent_itemsets)}\")\n",
    "        all_frequent_itemsets.extend(frequent_itemsets)\n",
    "\n",
    "        # Step 6: Move to the next level\n",
    "        k += 1\n",
    "\n",
    "    return all_frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "baa97098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_with_hash_tree(matrix, min_support, hash_func, verbose=False):\n",
    "    num_transactions, num_items = matrix.shape\n",
    "\n",
    "    support = matrix.sum(axis=0)\n",
    "    single_items = {index: support[index] for index in range(num_items)}\n",
    "    single_items = {k: v for k, v in single_items.items() if v >= min_support}\n",
    "    frequent_itemsets = [tuple([i]) for i in single_items.keys()]\n",
    "    all_frequent_itemsets = frequent_itemsets[:]\n",
    "\n",
    "    k = 2\n",
    "    while frequent_itemsets:\n",
    "        candidates = create_candidates(frequent_itemsets, k)\n",
    "        tree = HashTree(k, hash_func)\n",
    "        tree.insert_candidates(candidates)\n",
    "\n",
    "        for row in matrix:\n",
    "            transaction = tuple(i for i, bit in enumerate(row) if bit == 1)\n",
    "            if len(transaction) >= k:\n",
    "                tree.count_occurrences(transaction)\n",
    "\n",
    "        frequent_itemsets = tree.get_supported_candidates(min_support)\n",
    "        all_frequent_itemsets.extend(frequent_itemsets)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{k}-itemsets above min_support: {len(frequent_itemsets)}\")\n",
    "        k += 1\n",
    "\n",
    "    return all_frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "84a2af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_search_space(n):\n",
    "    return sum(comb(n, k) for k in range(1, n + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "dc77581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(matrix, min_support, alg, hash = None, verbose=False):\n",
    "    print(f\"Evaluate algorithm {alg.__name__}\")\n",
    "    if not hash:\n",
    "        start = time.time()\n",
    "    if hash:\n",
    "        print(\"Hash function used: \" + str(hash.__name__))\n",
    "        start = time.time()\n",
    "        frequent_itemsets = alg(matrix, min_support, hash, verbose)\n",
    "    else:\n",
    "        frequent_itemsets = alg(matrix, min_support, verbose)\n",
    "    end = time.time()\n",
    "    print(f\"Time: {end - start} seconds\")\n",
    "    print(f\"Number of frequent itemsets: {len(frequent_itemsets)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d2394d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_with_hash_tree_analysis(matrix, min_support, hash_func, verbose=False):\n",
    "    num_transactions, num_items = matrix.shape\n",
    "    support = matrix.sum(axis=0)\n",
    "    single_items = {index: support[index] for index in range(num_items)}\n",
    "    single_items = {k: v for k, v in single_items.items() if v >= min_support}\n",
    "    frequent_itemsets = [tuple([i]) for i in single_items.keys()]\n",
    "    all_frequent_itemsets = frequent_itemsets[:]\n",
    "\n",
    "    k = 2\n",
    "    total_candidates = 0\n",
    "    total_false_alarms = 0\n",
    "    while frequent_itemsets:\n",
    "        candidates = create_candidates(frequent_itemsets, k)\n",
    "        total_candidates += len(candidates)\n",
    "        tree = HashTree(k, hash_func=hash_func)\n",
    "        tree.insert_candidates(candidates)\n",
    "\n",
    "        for row in matrix:\n",
    "            transaction = tuple(i for i, bit in enumerate(row) if bit == 1)\n",
    "            if len(transaction) >= k:\n",
    "                tree.count_occurrences(transaction)\n",
    "\n",
    "        frequent_itemsets = tree.get_supported_candidates(min_support)\n",
    "        total_false_alarms += len(candidates) - len(frequent_itemsets)\n",
    "        all_frequent_itemsets.extend(frequent_itemsets)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{k}-itemsets: {len(frequent_itemsets)}\")\n",
    "        k += 1\n",
    "\n",
    "    if 'tree' in locals():\n",
    "        leaf_count, leaf_sizes = tree.get_leaf_statistics()\n",
    "    else:\n",
    "        leaf_count, leaf_sizes = 0, []\n",
    "    search_space = total_search_space(num_items)\n",
    "    pruning_rate = (1 - total_candidates / search_space) * 100\n",
    "    false_alarm_rate = (total_false_alarms / total_candidates) * 100 if total_candidates else 0\n",
    "    freq_percent = (len(all_frequent_itemsets) / search_space) * 100\n",
    "\n",
    "    return {\n",
    "        'frequent_itemsets': all_frequent_itemsets,\n",
    "        'leaf_count': leaf_count,\n",
    "        'leaf_sizes': leaf_sizes,\n",
    "        'search_space': search_space,\n",
    "        'total_candidates': total_candidates,\n",
    "        'pruning_rate': pruning_rate,\n",
    "        'false_alarm_rate': false_alarm_rate,\n",
    "        'frequent_percent': freq_percent\n",
    "    }\n",
    "\n",
    "def evaluate_analysis(matrix, min_support, hash_func):\n",
    "    start = time.time()\n",
    "    results = apriori_with_hash_tree_analysis(matrix, min_support, hash_func, verbose=True)\n",
    "    end = time.time()\n",
    "\n",
    "    print(\"\\nEvaluation Summary:\")\n",
    "    print(f\"Time taken: {end - start:.4f} seconds\")\n",
    "    print(f\"Number of frequent itemsets: {len(results['frequent_itemsets'])}\")\n",
    "    print(f\"Frequent % of total search space: {results['frequent_percent']:.4f}%\")\n",
    "    print(f\"Pruning rate: {results['pruning_rate']:.2f}%\")\n",
    "    print(f\"False alarm rate: {results['false_alarm_rate']:.2f}%\")\n",
    "    print(f\"Number of leaf nodes: {results['leaf_count']}\")\n",
    "    print(f\"Average leaf size: {np.mean(results['leaf_sizes']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "dfe9a488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate algorithm apriori_using_indices\n",
      "Time: 0.10713887214660645 seconds\n",
      "Number of frequent itemsets: 465\n",
      "\n",
      "Evaluate algorithm apriori_with_hash_tree\n",
      "Hash function used: simple_hash_func\n",
      "Time: 0.4795854091644287 seconds\n",
      "Number of frequent itemsets: 465\n",
      "\n",
      "Evaluate algorithm apriori_with_hash_tree\n",
      "Hash function used: hash_5\n",
      "Time: 0.5210635662078857 seconds\n",
      "Number of frequent itemsets: 465\n",
      "\n",
      "Evaluate algorithm apriori_with_hash_tree\n",
      "Hash function used: knuth_hash_func\n",
      "Time: 0.7660059928894043 seconds\n",
      "Number of frequent itemsets: 465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def simple_hash_func(x):\n",
    "    return x % 1\n",
    "\n",
    "def hash_5(x):\n",
    "    return hash(x) % 5\n",
    "\n",
    "def knuth_hash_func(x):\n",
    "    return (x * 2654435761) % 2**32 % 7\n",
    "\n",
    "matrix = generate_binary_matrix(1000, 30, 0.5)\n",
    "support_ratio = 0.2\n",
    "min_support = support_ratio * matrix.shape[0]\n",
    "evaluate(matrix, min_support, apriori_using_indices)\n",
    "evaluate(matrix, min_support, apriori_with_hash_tree, simple_hash_func)\n",
    "evaluate(matrix, min_support, apriori_with_hash_tree, hash_5)\n",
    "evaluate(matrix, min_support, apriori_with_hash_tree, knuth_hash_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "04f01c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-itemsets: 435\n",
      "3-itemsets: 0\n",
      "\n",
      "Evaluation Summary:\n",
      "Time taken: 0.4942 seconds\n",
      "Number of frequent itemsets: 465\n",
      "Frequent % of total search space: 0.0000%\n",
      "Pruning rate: 100.00%\n",
      "False alarm rate: 90.32%\n",
      "Number of leaf nodes: 1\n",
      "Average leaf size: 4060.00\n",
      "2-itemsets: 435\n",
      "3-itemsets: 0\n",
      "\n",
      "Evaluation Summary:\n",
      "Time taken: 0.5017 seconds\n",
      "Number of frequent itemsets: 465\n",
      "Frequent % of total search space: 0.0000%\n",
      "Pruning rate: 100.00%\n",
      "False alarm rate: 90.32%\n",
      "Number of leaf nodes: 125\n",
      "Average leaf size: 32.48\n",
      "2-itemsets: 435\n",
      "3-itemsets: 0\n",
      "\n",
      "Evaluation Summary:\n",
      "Time taken: 0.7386 seconds\n",
      "Number of frequent itemsets: 465\n",
      "Frequent % of total search space: 0.0000%\n",
      "Pruning rate: 100.00%\n",
      "False alarm rate: 90.32%\n",
      "Number of leaf nodes: 197\n",
      "Average leaf size: 20.61\n"
     ]
    }
   ],
   "source": [
    "evaluate_analysis(matrix, min_support, hash_func=simple_hash_func)\n",
    "evaluate_analysis(matrix, min_support, hash_func=hash_5)\n",
    "evaluate_analysis(matrix, min_support, hash_func=knuth_hash_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8ffbf5",
   "metadata": {},
   "source": [
    "We have observed that even though in theory the hash approach is quicker than the \"Brute force\" approach, in Python, the apriori_using_indices usually finishes quicker than the hash tree. On the one hand, the reason is that apriori_using_indices uses the vectorization from numpy, which is very fast (thanks to it being implemented in C and not in Python): mask = matrix[:, indices].all(axis=1), on the other hand, the hash tree approach suffers from the fact that it has to construct a new hash tree for each value of 'k'. For instance, with a matrix consisting of 500 transactions, each of which randomly can have at most 10 elements, with the probability 50% of containing an element, we got the following results: for the \"normal\" approach, the execution time was: 0.0051 seconds, for the hash tree approach with the hashing function h(x) = x % 1, the time was 0.031 seconds, using the hash function h(x) = hash(x) % 5 we obtained 0.021 seconds and for the knuth_hash_func we obtained 0.016 seconds. We have also noticed that the hash function we choose greately influences how the tree is constructed and implicitly, the number and size of leaves. For instance, with the example we have desribed above, for our first hash function, we obtained a single leaf with 120 itemsets. On the other hand, the second hash function gave us 79 leaves with an average leaf size of 1.52, meanwhile the third gave us 47 leaves with an average leaf size of 2.55. On the other hand, the hash function does not influence the false alarm rate, the pruning rate and the frequent % of the total search space. For the mentioned example, we have obtained a frequency percent of the total search space of 5.37%, a pruning rate of 83.87% and a false alarm rate of 72.73%. We have also noticed that with the increase of the number of columns comes the increase of the pruning rate and the false alarm rate and the decrease of the frequency of the itemsets from the total search space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c650ee6a",
   "metadata": {},
   "source": [
    "Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6c24c450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_transaction_dataset(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    transactions = [list(map(int, line.strip().split())) for line in lines if line.strip()]\n",
    "    return transactions\n",
    "\n",
    "def transactions_to_matrix(transactions):\n",
    "    all_items = sorted(set(item for tx in transactions for item in tx))\n",
    "    item_index = {item: idx for idx, item in enumerate(all_items)}\n",
    "    matrix = np.zeros((len(transactions), len(all_items)), dtype=int)\n",
    "    for i, tx in enumerate(transactions):\n",
    "        for item in tx:\n",
    "            matrix[i][item_index[item]] = 1\n",
    "    return matrix, item_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d7239072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Benchmark for: chess.dat.txt =====\n",
      "→ Custom Apriori with Hash Tree:\n",
      "Time: 32.4214s | Frequent itemsets: 9\n",
      "→ mlxtend Apriori:\n",
      "Time: 0.0028s | Frequent itemsets: 9\n",
      "\n",
      "===== Benchmark for: mushroom.dat.txt =====\n",
      "→ Custom Apriori with Hash Tree:\n",
      "Time: 0.4564s | Frequent itemsets: 1\n",
      "→ mlxtend Apriori:\n",
      "Time: 0.0031s | Frequent itemsets: 1\n",
      "\n",
      "===== Benchmark for: retail.dat.txt =====\n",
      "→ Custom Apriori with Hash Tree:\n",
      "Time: 3.9010s | Frequent itemsets: 0\n",
      "→ mlxtend Apriori:\n",
      "Time: 1.5955s | Frequent itemsets: 0\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import apriori as mlxtend_apriori\n",
    "\n",
    "def benchmark_dataset(filepath, min_support_ratio=0.01):\n",
    "    print(f\"\\n===== Benchmark for: {filepath} =====\")\n",
    "    transactions = load_transaction_dataset(filepath)\n",
    "    matrix, item_index = transactions_to_matrix(transactions)\n",
    "    min_support = int(min_support_ratio * len(matrix))\n",
    "    \n",
    "    print(\"→ Custom Apriori with Hash Tree:\")\n",
    "    start = time.time()\n",
    "    custom_result = apriori_with_hash_tree(matrix, min_support, hash_5)\n",
    "    custom_time = time.time() - start\n",
    "    print(f\"Time: {custom_time:.4f}s | Frequent itemsets: {len(custom_result)}\")\n",
    "\n",
    "    print(\"→ mlxtend Apriori:\")\n",
    "    df = pd.DataFrame(matrix, columns=[f'item_{i}' for i in range(matrix.shape[1])]).astype(bool)\n",
    "    start = time.time()\n",
    "    mlxtend_result = mlxtend_apriori(df, min_support=min_support_ratio, use_colnames=True)\n",
    "    mlxtend_time = time.time() - start\n",
    "    print(f\"Time: {mlxtend_time:.4f}s | Frequent itemsets: {len(mlxtend_result)}\")\n",
    "\n",
    "    return {\n",
    "        'dataset': filepath,\n",
    "        'custom_time': custom_time,\n",
    "        'mlxtend_time': mlxtend_time,\n",
    "        'custom_count': len(custom_result),\n",
    "        'mlxtend_count': len(mlxtend_result)\n",
    "    }\n",
    "\n",
    "results = []\n",
    "for file in ['chess.dat.txt', 'mushroom.dat.txt', 'retail.dat.txt']:\n",
    "    results.append(benchmark_dataset(file, min_support_ratio=0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4f2b82",
   "metadata": {},
   "source": [
    "With our benchmarking, we have noticed that the mlxtend implementation of apriori is considerably more efficient than our implementation. For instance, using the chess dataset, it took 31.89 seconds for our implementation to find 9 frequent datasets, meanwhile the same thing only took 0.0044 seconds for the mlxtend algorithm. Similarly, in the mushroom dataset, it took our algorithm 0.45 seconds to find 1 frequent itemset, which was found in 0.0031 seconds by mlxtend. In the retail dataset, where there were no frequent itemsets found due to the high min_support_ration, it took our algorithm 3.90 seconds to finish and it took 1.59 seconds for the mlxtend algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python DMenv",
   "language": "python",
   "name": "dmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
